{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tweets (excludes replies): 6501\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:n_documents: 4275\n",
      "INFO:guidedlda:vocab_size: 2911\n",
      "INFO:guidedlda:n_words: 46965\n",
      "INFO:guidedlda:n_topics: 10\n",
      "INFO:guidedlda:n_iter: 200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tweets within 10 topics: 4275\n",
      "The number of other tweets: 2226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:<0> log likelihood: -520717\n",
      "INFO:guidedlda:<10> log likelihood: -371787\n",
      "INFO:guidedlda:<20> log likelihood: -359300\n",
      "INFO:guidedlda:<30> log likelihood: -355065\n",
      "INFO:guidedlda:<40> log likelihood: -352176\n",
      "INFO:guidedlda:<50> log likelihood: -351119\n",
      "INFO:guidedlda:<60> log likelihood: -350436\n",
      "INFO:guidedlda:<70> log likelihood: -349057\n",
      "INFO:guidedlda:<80> log likelihood: -348911\n",
      "INFO:guidedlda:<90> log likelihood: -348454\n",
      "INFO:guidedlda:<100> log likelihood: -348266\n",
      "INFO:guidedlda:<110> log likelihood: -347980\n",
      "INFO:guidedlda:<120> log likelihood: -347744\n",
      "INFO:guidedlda:<130> log likelihood: -347479\n",
      "INFO:guidedlda:<140> log likelihood: -347126\n",
      "INFO:guidedlda:<150> log likelihood: -346729\n",
      "INFO:guidedlda:<160> log likelihood: -346768\n",
      "INFO:guidedlda:<170> log likelihood: -347136\n",
      "INFO:guidedlda:<180> log likelihood: -346490\n",
      "INFO:guidedlda:<190> log likelihood: -346690\n",
      "INFO:guidedlda:<199> log likelihood: -346682\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Topic: 0\n",
      "0.0418*food  0.0304*chang  0.0224*climat  0.0204*system  0.018*sustain  0.0171*world  0.0169*anim  0.0145*global  0.0135*health  0.0124*help  0.0116*impact  0.0112*human  0.011*industri  0.0108*suppli  0.0104*environment  0.0098*product  0.0096*feed  0.0092*need  0.0084*pandem  0.0082*agricultur  0.008*solut  0.008*time  0.0078*chain  0.0078*challeng  0.0073*grow  0.0073*reduc  0.0071*potenti  0.0065*creat  0.0063*earth  0.0061*today  0.0059*could  0.0059*mission  0.0059*issu  0.0057*demand  0.0055*environ  0.0055*news  0.0055*live  0.0055*way  0.0053*consum  0.0053*good  \n",
      "\n",
      "Topic: 1\n",
      "0.0702*anim  0.0528*cell  0.0304*without  0.0282*produc  0.0219*grow  0.0214*real  0.0201*base  0.0166*slaughter  0.0164*make  0.0153*free  0.0136*startup  0.0131*use  0.0126*need  0.0118*product  0.0116*chicken  0.0116*develop  0.0106*cow  0.0106*protein  0.0103*farm  0.0103*plant  0.0096*grown  0.0096*rais  0.0091*compani  0.0078*learn  0.0075*cultur  0.0073*like  0.0073*whole  0.0073*cost  0.0065*meet  0.0065*crustacean  0.006*kill  0.006*muscl  0.0055*peopl  0.0055*directli  0.0055*convent  0.0055*save  0.005*look  0.005*qualiti  0.005*medium  0.005*made  \n",
      "\n",
      "Topic: 2\n",
      "0.0301*plant  0.0253*base  0.0242*industri  0.0226*market  0.0214*year  0.0182*food  0.0147*protein  0.0131*altern  0.0124*consum  0.0114*like  0.0108*product  0.0102*billion  0.0089*global  0.0089*demand  0.0087*farm  0.0087*agricultur  0.0087*need  0.0085*invest  0.0081*eat  0.0081*report  0.0075*water  0.0072*antibiot  0.0068*peopl  0.0068*land  0.0068*next  0.0066*vegan  0.0066*come  0.0064*anim  0.0062*find  0.006*shrimp  0.0058*scale  0.0058*opportun  0.0054*consumpt  0.0054*trend  0.0052*sale  0.005*would  0.005*much  0.0048*even  0.0048*beef  0.0048*take  \n",
      "\n",
      "Topic: 3\n",
      "0.0455*fish  0.0343*seafood  0.0241*ocean  0.0226*base  0.019*cell  0.0182*product  0.0161*sustain  0.0139*food  0.0109*market  0.0105*thing  0.0088*also  0.0083*think  0.0078*healthi  0.0075*well  0.0075*could  0.0075*regul  0.0073*safe  0.0073*wild  0.0071*cubiq  0.0071*move  0.0071*improv  0.0068*industri  0.0066*provid  0.0066*produc  0.0063*work  0.0063*smart  0.0063*help  0.0063*bring  0.0061*cellular  0.0058*bluenalu  0.0058*finless  0.0056*know  0.0056*would  0.0056*tradit  0.0056*term  0.0056*import  0.0054*recent  0.0054*plastic  0.0054*consum  0.0054*aquacultur  \n",
      "\n",
      "Topic: 4\n",
      "0.0409*grown  0.0395*first  0.0386*steak  0.0303*cell  0.0299*tast  0.0285*lab  0.0253*chicken  0.0202*base  0.0198*world  0.0184*farm  0.0168*burger  0.0161*aleph  0.0122*beef  0.0106*product  0.0101*isra  0.0099*made  0.0097*like  0.0085*free  0.008*textur  0.0078*make  0.0076*slaughter  0.0076*creat  0.0076*produc  0.0074*duck  0.0071*cook  0.0069*vegan  0.0067*israel  0.0067*articl  0.0064*good  0.0064*plant  0.0064*delici  0.0057*space  0.0057*experi  0.0055*test  0.0055*time  0.0053*look  0.0053*shape  0.0051*kitchen  0.0051*salmon  0.0048*flavor  \n",
      "\n",
      "Topic: 5\n",
      "0.0346*base  0.0313*compani  0.0307*cell  0.0285*invest  0.0249*million  0.0249*rais  0.0241*announc  0.0235*startup  0.0229*fund  0.0175*food  0.0171*first  0.0153*round  0.0147*product  0.0133*seri  0.0121*start  0.0119*seafood  0.0117*plant  0.0106*singapor  0.0102*investor  0.0092*protein  0.0092*excit  0.0084*seed  0.0084*industri  0.008*launch  0.0074*futur  0.0074*includ  0.0072*space  0.0072*close  0.0072*congratul  0.007*meat  0.0064*full  0.0062*team  0.0056*thrill  0.0056*ventur  0.0054*today  0.0054*farm  0.0052*isra  0.005*capit  0.005*bring  0.005*break  \n",
      "\n",
      "Topic: 6\n",
      "0.0375*anim  0.0324*thank  0.0282*make  0.0273*futur  0.024*help  0.0214*want  0.0186*without  0.0182*better  0.017*world  0.0163*support  0.0156*join  0.0145*check  0.0142*peopl  0.0135*free  0.0135*meat  0.0133*planet  0.0114*food  0.0112*campaign  0.011*slaughter  0.0107*memphi  0.0105*pleas  0.0105*love  0.0103*tast  0.01*work  0.0093*great  0.0086*environ  0.0086*movement  0.0082*delici  0.0077*spread  0.0077*eat  0.0072*like  0.007*word  0.007*think  0.0068*happi  0.0065*real  0.0065*sustain  0.0063*contribut  0.0063*healthier  0.0058*kill  0.0058*interest  \n",
      "\n",
      "Topic: 7\n",
      "0.0279*futur  0.026*speak  0.026*food  0.0246*join  0.0174*discuss  0.0159*confer  0.0145*panel  0.0124*live  0.0122*ceo  0.0113*check  0.011*event  0.0105*summit  0.0105*innov  0.0099*talk  0.0091*th  0.0085*excit  0.0085*founder  0.0084*present  0.0084*tomorrow  0.0082*today  0.008*next  0.0077*week  0.0073*watch  0.0073*co  0.007*cellular  0.007*technolog  0.007*industri  0.0068*protein  0.0068*tech  0.0066*tune  0.0065*world  0.0061*share  0.0059*brian  0.0059*host  0.0058*take  0.0058*regist  0.0058*catch  0.0058*hear  0.0056*agricultur  0.0054*follow  \n",
      "\n",
      "Topic: 8\n",
      "0.0453*thank  0.021*check  0.0194*featur  0.0194*year  0.0177*meat  0.0157*excit  0.0153*shiok  0.0144*read  0.0136*articl  0.0136*interview  0.0128*compani  0.0126*look  0.0124*great  0.0122*latest  0.0113*amaz  0.0111*post  0.0109*happi  0.0103*shrimp  0.0103*news  0.0095*stori  0.0095*forward  0.0089*share  0.0085*blog  0.0085*episod  0.0082*includ  0.008*video  0.0078*list  0.0078*mani  0.0072*last  0.0072*much  0.0072*support  0.0068*love  0.0064*week  0.006*part  0.006*listen  0.0056*podcast  0.0054*full  0.0054*startup  0.0054*cover  0.0054*time  \n",
      "\n",
      "Topic: 9\n",
      "0.0387*team  0.0289*cell  0.0195*work  0.0186*excit  0.0168*food  0.0164*join  0.0152*research  0.015*look  0.0143*base  0.0125*scienc  0.0121*bring  0.0113*meat  0.0109*innov  0.01*world  0.0098*develop  0.009*founder  0.009*welcom  0.009*scientist  0.0086*hire  0.0082*industri  0.008*appli  0.0078*co  0.0078*proud  0.0078*sustain  0.0072*engin  0.007*help  0.0066*today  0.0064*great  0.0064*seafood  0.0061*interest  0.0059*congrat  0.0057*make  0.0055*togeth  0.0055*open  0.0055*member  0.0055*grow  0.0053*stem  0.0051*celebr  0.0051*part  0.0051*product  topic_number\n",
      "0    431\n",
      "1    364\n",
      "2    427\n",
      "3    286\n",
      "4    394\n",
      "5    449\n",
      "6    429\n",
      "7    577\n",
      "8    542\n",
      "9    376\n",
      "Name: number, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#read data(exclude replies) \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import SnowballStemmer\n",
    "import pandas as pd\n",
    "\n",
    "text = pd.read_csv('Jan6(excludes replies).csv')\n",
    "print('The number of tweets (excludes replies):',len(text))\n",
    "#Text cleaning\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import re\n",
    "stemmer = SnowballStemmer('english')\n",
    "porter = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += ['cultured','clean','cultivated','meat']\n",
    "b = []\n",
    "for i,u in text.iterrows():\n",
    "    a = []\n",
    "    word =''\n",
    "    for words in str(u['tweets']).split():\n",
    "        if '@' not in words: #remove @users\n",
    "            if '#' not in words:\n",
    "                if 'http' not in words: #remove URLs\n",
    "                    if'&amp' not in words: #remove symbol\n",
    "                        words = words.lower()# lower form\n",
    "                        words = re.sub(r'[^a-zA-Z]', ' ', words) #replace non-alphabets characters with space. From \"can't\" to \"can t\"\n",
    "                        if len(words)>3:\n",
    "                            word += (words+' ')\n",
    "    doc = ''\n",
    "    for token in word.split():\n",
    "        if len(token) >1:\n",
    "            if token not in stop_words:\n",
    "                token = porter.stem(token) #root form\n",
    "                doc += (token+' ')\n",
    "    b.append(doc)\n",
    "text['processed']=[i for i in b]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import guidedlda\n",
    "import numpy as np\n",
    "import gensim\n",
    "topic_tweets=[]\n",
    "other_tweets=[]\n",
    "\n",
    "keywords_list = ['environ','environment','carbon','climat','greenhous','emiss', #Environmental Impact\n",
    "                   'plant','vegan','vegetarian','animal','without','slaughter',\n",
    "                   'cruelti','welfar','cell','cellular','anim','kill',# Animal Welfare\n",
    "                  'antibiot','antimicrobi','resist','safeti','contamin','consumpt','popul',\n",
    "                  'demand','secur','wast','industri','suppli','convent','regular','system','market','consum','store','groceri', #Food System\n",
    "                  'seafood','sea','ocean','fish','aquacultur','shrimp',#Seafood\n",
    "                  'delici','tast','tender','textur','juici','test','chicken','duck','poultri','egg',#Poultry\n",
    "                  'seri','fund','funder','rais','invest','investor','dollar','pound','announc', \n",
    "                  'happi','thrill','excit', #Announcement\n",
    "                  'thank','thankyou','support','shoutout','help','shout', #Appreciation\n",
    "                  'confer','regist','live','symposium','stream','livestream','ceo','speak','summit', #conference and summit\n",
    "                  'check','post','paper','interview','articl','blog','news','break','episod','discuss','vote','stori' #Media\n",
    "                  'hire','join','team','career']#Hiring Information\n",
    "for index,i in text.iterrows():\n",
    "    count = 0 \n",
    "    for word in i['processed'].split():\n",
    "        if word in keywords_list:\n",
    "            count+=1 #if any keyword occur in a tweet, the tweet may belong to one of the above themes\n",
    "    if count !=0: #if none of the keywords occur in a tweet, the tweet doesnot belong to any themes we found\n",
    "        topic_tweets.append(i['processed'])\n",
    "    else:\n",
    "        other_tweets.append(i['processed'])\n",
    "print('The number of tweets within 10 topics:',len(topic_tweets))\n",
    "print('The number of other tweets:',len(other_tweets))\n",
    "\n",
    "\n",
    "seed_topic_list = [['environ','environment','carbon','climat','greenhous','emiss'], #Environmental Impact\n",
    "                   ['plant','vegan','vegetarian','animal','without','slaughter',\n",
    "                   'cruelti','welfar','cell','cellular','anim','kill'],# Animal Welfare\n",
    "                  ['antibiot','antimicrobi','resist','safeti','contamin','consumpt','popul','demand','secur','wast',\n",
    "                   'industri','suppli','convent','regular','system','market','consum','store','groceri'], #Food System\n",
    "                  ['seafood','sea','ocean','fish','aquacultur','shrimp'],#Seafood\n",
    "                  ['delici','tast','tender','textur','juici','test','chicken','duck','poultri','egg'],#Poultry\n",
    "                  ['seri','fund','funder','rais','invest','investor','dollar','pound','announc', \n",
    "                  'happi','thrill','excit','honor',], #Announcement\n",
    "                  ['thank','thankyou','support','shoutout','help','shout'], #Appreciation\n",
    "                  ['confer','regist','live','symposium','stream','livestream','speak','summit'], #conference and summit\n",
    "                  ['check','post','paper','interview','articl','blog','news','break','episod','discuss','vote','stori'], #Media\n",
    "                  ['hire','join','team','career']]#Hiring Information\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "model = guidedlda.GuidedLDA(n_topics=10,n_iter=200,random_state=1,refresh=10,alpha=0.1,eta=0.01)\n",
    "vectorizer = CountVectorizer(min_df = 2)\n",
    "X = vectorizer.fit_transform(topic_tweets)\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "word2id = dict((v,idx) for idx,v in enumerate(vocab))\n",
    "seed_topics = {}\n",
    "for t_id, st in enumerate(seed_topic_list):\n",
    "    for word in st:\n",
    "        seed_topics[word2id[word]] = t_id\n",
    "\n",
    "model.fit(X.toarray(),seed_topics=seed_topics,seed_confidence=0.35)\n",
    "topic_word = model.topic_word_\n",
    "n_top_words = 40\n",
    "vocab = tuple(vocab)\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    print('\\n')\n",
    "    print('Topic:',i)\n",
    "    words_probability = np.array(-topic_dist)\n",
    "    for index in range(n_top_words):\n",
    "        print(round(abs(np.sort(words_probability))[:(n_top_words)][index],4),'*',\n",
    "              np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1][index],sep='',end='  ')\n",
    "\n",
    "X = X.toarray()\n",
    "doc_topic = model.transform(X)\n",
    "topic_number = []\n",
    "number = []\n",
    "for i in range(len(doc_topic)):\n",
    "    topic_number.append(doc_topic[i].argmax())#得到每篇tweets對應概率最高的topic\n",
    "    number.append('1')\n",
    "data = pd.DataFrame({'Company':[i for i in topic_tweets_Company],\n",
    "                    'number':[i for i in number],\n",
    "                     'topic_number':[i for i in topic_number]})\n",
    "print(data.groupby(['Company','topic_number'])['number'].count())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
