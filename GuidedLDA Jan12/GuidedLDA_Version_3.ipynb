{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tweets (excludes replies): 6501\n"
     ]
    }
   ],
   "source": [
    "#read data(exclude replies) \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import SnowballStemmer\n",
    "import pandas as pd\n",
    "\n",
    "text = pd.read_csv('Jan6(excludes replies).csv')\n",
    "print('The number of tweets (excludes replies):',len(text))\n",
    "#Text cleaning\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import re\n",
    "stemmer = SnowballStemmer('english')\n",
    "porter = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += ['cultured','clean','cultivated','meat']\n",
    "b = []\n",
    "for i,u in text.iterrows():\n",
    "    a = []\n",
    "    word =''\n",
    "    for words in str(u['tweets']).split():\n",
    "        if '@' not in words: #remove @users\n",
    "            if '#' not in words:\n",
    "                if 'http' not in words: #remove URLs\n",
    "                    if'&amp' not in words: #remove symbol\n",
    "                        words = words.lower()# lower form\n",
    "                        words = re.sub(r'[^a-zA-Z]', ' ', words) #replace non-alphabets characters with space. From \"can't\" to \"can t\"\n",
    "                        if len(words)>3:\n",
    "                            word += (words+' ')\n",
    "    doc = ''\n",
    "    for token in word.split():\n",
    "        if token not in stop_words:\n",
    "            token = porter.stem(token) #root form\n",
    "            doc += (token+' ')\n",
    "    b.append(doc)\n",
    "text['processed']=[i for i in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigram\n",
    "unigram = []\n",
    "unigram_list = []\n",
    "for index, i in text.iterrows():\n",
    "    unigram=''\n",
    "    for word in i['processed'].split():\n",
    "        unigram+= word+' '\n",
    "    unigram_list.append(unigram)\n",
    "\n",
    "\n",
    "import guidedlda\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "seed_topic_list = [['environ','environment','carbon','climat','greenhous','emiss'], #Environmental Impact\n",
    "                   ['plant','vegan','vegetarian','animal','without','slaughter',\n",
    "                   'cruelti','welfar'],# Animal Welfare\n",
    "                  ['antibiot','antimicrobi','resist','safeti','contamin','consumpt','popul',\n",
    "                  'demand','secur','wast','industri','suppli','convent','regular','system','market','consum','store','groceri'], #Food System\n",
    "                  ['seafood','sea','ocean','fish','aquacultur','shrimp'],#Seafood\n",
    "                  ['tast','tender','textur','juici','test','chicken','duck','poultri','egg'],#Poultry\n",
    "                  ['seri','fund','funder','rais','invest','investor','dollar','pound','announc', \n",
    "                  'happi','thrill','excit'], #Announcement\n",
    "                  ['thank','thankyou','support','shoutout','help'], #Appreciation\n",
    "                  ['confer','regist','live','symposium','stream','livestream','summit'], #conference and summit\n",
    "                  ['check','post','paper','interview','articl','blog','news','break'], #Media\n",
    "                  ['hire','join','team']]#Hiring Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:n_documents: 6501\n",
      "INFO:guidedlda:vocab_size: 3621\n",
      "INFO:guidedlda:n_words: 60068\n",
      "INFO:guidedlda:n_topics: 10\n",
      "INFO:guidedlda:n_iter: 200\n",
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "INFO:guidedlda:<0> log likelihood: -677657\n",
      "INFO:guidedlda:<10> log likelihood: -488456\n",
      "INFO:guidedlda:<20> log likelihood: -472934\n",
      "INFO:guidedlda:<30> log likelihood: -466380\n",
      "INFO:guidedlda:<40> log likelihood: -462785\n",
      "INFO:guidedlda:<50> log likelihood: -460274\n",
      "INFO:guidedlda:<60> log likelihood: -458965\n",
      "INFO:guidedlda:<70> log likelihood: -458062\n",
      "INFO:guidedlda:<80> log likelihood: -456967\n",
      "INFO:guidedlda:<90> log likelihood: -456704\n",
      "INFO:guidedlda:<100> log likelihood: -455979\n",
      "INFO:guidedlda:<110> log likelihood: -455437\n",
      "INFO:guidedlda:<120> log likelihood: -455343\n",
      "INFO:guidedlda:<130> log likelihood: -454906\n",
      "INFO:guidedlda:<140> log likelihood: -454991\n",
      "INFO:guidedlda:<150> log likelihood: -454700\n",
      "INFO:guidedlda:<160> log likelihood: -454300\n",
      "INFO:guidedlda:<170> log likelihood: -454075\n",
      "INFO:guidedlda:<180> log likelihood: -454155\n",
      "INFO:guidedlda:<190> log likelihood: -453996\n",
      "INFO:guidedlda:<199> log likelihood: -453597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Topic: 0\n",
      "0.0255*chang  0.0255*food  0.0243*anim  0.0194*world  0.0182*sustain  0.0159*climat  0.0145*human  0.0117*need  0.0114*feed  0.0101*health  0.0101*global  0.01*planet  0.0097*environment  0.0096*solut  0.0088*grow  0.0088*peopl  0.0088*agricultur  0.0083*system  0.0081*product  0.0077*reduc  0.0077*impact  0.0074*industri  0.0072*save  0.0071*help  0.0068*environ  0.0068*earth  0.0068*eat  0.0068*produc  0.0067*use  0.0065*could  \n",
      "\n",
      "Topic: 1\n",
      "0.0571*anim  0.026*without  0.0255*make  0.0216*cell  0.0213*base  0.0176*free  0.0152*real  0.0152*plant  0.0147*grow  0.0142*slaughter  0.0135*food  0.0111*want  0.0111*futur  0.0106*burger  0.0103*need  0.0098*tast  0.0096*produc  0.0096*eat  0.0093*peopl  0.0086*kill  0.0084*know  0.0083*help  0.0078*harm  0.0078*like  0.0078*vegan  0.0071*answer  0.0071*made  0.0068*cow  0.0064*better  0.0062*would  \n",
      "\n",
      "Topic: 2\n",
      "0.0285*product  0.0277*base  0.0209*market  0.0203*industri  0.0185*year  0.0183*cell  0.0176*plant  0.0135*seafood  0.0127*food  0.0106*consum  0.0103*protein  0.0095*scale  0.0091*like  0.0087*altern  0.0086*suppli  0.0079*take  0.0078*could  0.0077*fish  0.0073*next  0.0071*global  0.0068*chain  0.0065*demand  0.0064*today  0.006*time  0.0058*beyond  0.0058*report  0.005*import  0.0049*salmon  0.0049*wild  0.0048*bluenalu  \n",
      "\n",
      "Topic: 3\n",
      "0.0214*ocean  0.0197*fish  0.0173*cell  0.0156*shrimp  0.0151*sustain  0.0151*food  0.0132*product  0.0119*base  0.0108*delici  0.0099*tast  0.0099*make  0.0097*thank  0.0097*seafood  0.0091*contribut  0.0086*also  0.008*healthi  0.0078*cubiq  0.0076*gt  0.0073*well  0.0073*cook  0.0069*fat  0.0067*enjoy  0.0065*shiok  0.0063*chef  0.0063*protein  0.0061*work  0.0061*smart  0.0061*plastic  0.0056*great  0.0056*day  \n",
      "\n",
      "Topic: 4\n",
      "0.0623*grown  0.0479*first  0.0469*steak  0.0454*lab  0.034*cell  0.0288*farm  0.0263*world  0.0248*chicken  0.0187*aleph  0.0183*startup  0.0165*tast  0.0137*produc  0.0137*free  0.0131*isra  0.0119*slaughter  0.0117*beef  0.0108*compani  0.0102*made  0.0096*creat  0.0088*start  0.0087*rais  0.0085*space  0.0081*could  0.0079*year  0.0079*like  0.0077*meatbal  0.0069*ever  0.0069*real  0.0062*million  0.0062*anim  \n",
      "\n",
      "Topic: 5\n",
      "0.0324*invest  0.03*base  0.0293*compani  0.0217*fund  0.0196*announc  0.0193*rais  0.0193*startup  0.0172*cell  0.0159*million  0.0144*round  0.0131*food  0.0115*seri  0.0113*product  0.0113*protein  0.0111*investor  0.0111*plant  0.0107*start  0.01*close  0.0094*first  0.0091*singapor  0.0081*industri  0.0072*seed  0.007*meat  0.007*launch  0.0069*ventur  0.0069*seafood  0.0061*tech  0.0057*congratul  0.0057*includ  0.0057*capit  \n",
      "\n",
      "Topic: 6\n",
      "0.0321*thank  0.0287*great  0.0167*look  0.016*futur  0.0145*love  0.0142*good  0.0135*time  0.0135*work  0.0125*world  0.0111*make  0.0106*articl  0.0105*like  0.0103*forward  0.0101*realli  0.0101*check  0.01*think  0.01*excit  0.0093*piec  0.0088*talk  0.0088*friend  0.0086*peopl  0.0086*everyon  0.0083*amaz  0.0081*support  0.0079*chang  0.0078*well  0.0076*memphi  0.0073*much  0.0073*bring  0.0071*thing  \n",
      "\n",
      "Topic: 7\n",
      "0.0356*futur  0.0342*food  0.0214*join  0.0205*speak  0.0161*panel  0.0124*today  0.0119*event  0.0119*discuss  0.0119*week  0.0113*confer  0.011*live  0.011*protein  0.0107*next  0.0101*excit  0.01*th  0.01*agricultur  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:guidedlda:all zero row in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0098*tomorrow  0.0098*innov  0.0092*talk  0.0089*summit  0.0085*world  0.0085*present  0.0083*ceo  0.0079*meet  0.0077*cellular  0.0076*altern  0.0074*technolog  0.007*tune  0.0067*sign  0.0067*come  \n",
      "\n",
      "Topic: 8\n",
      "0.0228*founder  0.0225*featur  0.0176*co  0.0173*meat  0.0161*thank  0.0157*cell  0.0154*check  0.0154*compani  0.0152*interview  0.014*post  0.0137*read  0.0128*latest  0.0112*base  0.0105*great  0.0102*podcast  0.01*shiok  0.01*stori  0.0098*talk  0.0093*discuss  0.0092*watch  0.0088*innov  0.0086*list  0.0085*startup  0.0081*seafood  0.0079*food  0.0078*industri  0.0076*articl  0.0074*today  0.0074*video  0.0074*news  \n",
      "\n",
      "Topic: 9\n",
      "0.0321*team  0.0226*join  0.0204*food  0.0196*meat  0.0194*work  0.0188*excit  0.0151*make  0.0143*research  0.0136*year  0.0127*help  0.0125*look  0.0124*futur  0.0116*memphi  0.0105*sustain  0.0096*share  0.0094*support  0.0094*welcom  0.0093*hire  0.0089*scientist  0.0088*develop  0.0086*pleas  0.0083*proud  0.0082*happi  0.008*world  0.0074*appli  0.0071*build  0.0071*scienc  0.0069*want  0.0067*togeth  0.0066*innov  \n",
      "\n",
      "topic_number\n",
      "0    837\n",
      "1    664\n",
      "2    691\n",
      "3    405\n",
      "4    594\n",
      "5    533\n",
      "6    796\n",
      "7    744\n",
      "8    595\n",
      "9    641\n",
      "Name: number, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "model = guidedlda.GuidedLDA(n_topics=10,n_iter=200,random_state=1,refresh=10,alpha=0.1,eta=0.01)\n",
    "vectorizer = CountVectorizer(min_df = 2)\n",
    "X = vectorizer.fit_transform(text['processed'])\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "word2id = dict((v,idx) for idx,v in enumerate(vocab))\n",
    "seed_topics = {}\n",
    "for t_id, st in enumerate(seed_topic_list):\n",
    "    for word in st:\n",
    "        seed_topics[word2id[word]] = t_id\n",
    "\n",
    "model.fit(X.toarray(),seed_topics=seed_topics,seed_confidence=0.35)\n",
    "topic_word = model.topic_word_\n",
    "n_top_words = 30\n",
    "vocab = tuple(vocab)\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    print('\\n')\n",
    "    print('Topic:',i)\n",
    "    words_probability = np.array(-topic_dist)\n",
    "    for index in range(n_top_words):\n",
    "        print(round(abs(np.sort(words_probability))[:(n_top_words)][index],4),'*',\n",
    "              np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1][index],sep='',end='  ')\n",
    "print('\\n')    \n",
    "X = X.toarray()\n",
    "doc_topic = model.transform(X)\n",
    "topic_number = []\n",
    "number = []\n",
    "for i in range(len(doc_topic)-1):\n",
    "    topic_number.append(doc_topic[i].argmax())\n",
    "    number.append('1')\n",
    "data = pd.DataFrame(data=[i for i in topic_number],columns=['topic_number'])\n",
    "data['number'] = [i for i in number]\n",
    "print(data.groupby('topic_number')['number'].count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
