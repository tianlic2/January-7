{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tweets (excludes replies): 6501\n"
     ]
    }
   ],
   "source": [
    "#read data(exclude replies) \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import SnowballStemmer\n",
    "import pandas as pd\n",
    "\n",
    "text = pd.read_csv('Jan6(excludes replies).csv')\n",
    "print('The number of tweets (excludes replies):',len(text))\n",
    "#Text cleaning\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import re\n",
    "stemmer = SnowballStemmer('english')\n",
    "porter = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += ['cultured','clean','cultivated','meat']\n",
    "b = []\n",
    "for i,u in text.iterrows():\n",
    "    a = []\n",
    "    word =''\n",
    "    for words in str(u['tweets']).split():\n",
    "        if '@' not in words: #remove @users\n",
    "            if '#' not in words:\n",
    "                if 'http' not in words: #remove URLs\n",
    "                    if'&amp' not in words: #remove symbol\n",
    "                        words = words.lower()# lower form\n",
    "                        words = re.sub(r'[^a-zA-Z]', ' ', words) #replace non-alphabets characters with space. From \"can't\" to \"can t\"\n",
    "                        if len(words)>3:\n",
    "                            word += (words+' ')\n",
    "    doc = ''\n",
    "    for token in word.split():\n",
    "        if token not in stop_words:\n",
    "            token = porter.stem(token) #root form\n",
    "            doc += (token+' ')\n",
    "    b.append(doc)\n",
    "text['processed']=[i for i in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigram\n",
    "unigram = []\n",
    "unigram_list = []\n",
    "for index, i in text.iterrows():\n",
    "    unigram=''\n",
    "    for word in i['processed'].split():\n",
    "        unigram+= word+' '\n",
    "    unigram_list.append(unigram)\n",
    "\n",
    "\n",
    "import guidedlda\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "seed_topic_list = [['environ','environment','carbon','climat','greenhous','emiss'], #Environmental Impact\n",
    "                   ['plant','vegan','vegetarian','animal','without','slaughter',\n",
    "                   'cruelti','welfar'],# Animal Welfare\n",
    "                  ['antibiot','antimicrobi','resist','safeti','contamin','consumpt','popul',\n",
    "                  'demand','secur','wast','industri','suppli','convent','regular','system'], #Food System\n",
    "                  ['seafood','sea','ocean','fish','aquacultur','shrimp'],#Seafood\n",
    "                  ['delici','tast','tender','textur','juici','test','chicken','duck','poultri','egg'],#Poultry\n",
    "                  ['cell','stem','cultur','muscl','biolog','divis'], #Process\n",
    "                  ['seri','fund','funder','rais','invest','investor','dollar','pound','announc', \n",
    "                  'happi','thrill','excit'], #Announcement\n",
    "                  ['thank','thankyou','support','shoutout','help'], #Appreciation\n",
    "                  ['confer','regist','live','symposium','stream','livestream'], #conference and summit\n",
    "                  ['market','consum','store','groceri'], # Market\n",
    "                  ['check','post','paper','interview','articl','blog','news','break'], #Media\n",
    "                  ['hire','join','team']]#Hiring Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:n_documents: 6501\n",
      "INFO:guidedlda:vocab_size: 3621\n",
      "INFO:guidedlda:n_words: 60068\n",
      "INFO:guidedlda:n_topics: 12\n",
      "INFO:guidedlda:n_iter: 200\n",
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "INFO:guidedlda:<0> log likelihood: -694392\n",
      "INFO:guidedlda:<10> log likelihood: -490422\n",
      "INFO:guidedlda:<20> log likelihood: -474945\n",
      "INFO:guidedlda:<30> log likelihood: -467499\n",
      "INFO:guidedlda:<40> log likelihood: -463902\n",
      "INFO:guidedlda:<50> log likelihood: -461861\n",
      "INFO:guidedlda:<60> log likelihood: -460037\n",
      "INFO:guidedlda:<70> log likelihood: -458819\n",
      "INFO:guidedlda:<80> log likelihood: -457591\n",
      "INFO:guidedlda:<90> log likelihood: -456854\n",
      "INFO:guidedlda:<100> log likelihood: -456434\n",
      "INFO:guidedlda:<110> log likelihood: -455634\n",
      "INFO:guidedlda:<120> log likelihood: -455670\n",
      "INFO:guidedlda:<130> log likelihood: -455698\n",
      "INFO:guidedlda:<140> log likelihood: -455183\n",
      "INFO:guidedlda:<150> log likelihood: -455020\n",
      "INFO:guidedlda:<160> log likelihood: -454832\n",
      "INFO:guidedlda:<170> log likelihood: -454291\n",
      "INFO:guidedlda:<180> log likelihood: -454387\n",
      "INFO:guidedlda:<190> log likelihood: -453950\n",
      "INFO:guidedlda:<199> log likelihood: -453895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Topic: 0\n",
      "0.0271*chang 0.0255*food 0.0216*anim 0.0183*world 0.018*climat 0.0154*need 0.0144*sustain 0.0124*human 0.0116*solut 0.0113*health 0.0101*peopl 0.0095*produc 0.0092*environment 0.0088*global 0.0088*impact 0.0087*system 0.0087*agricultur 0.0085*reduc 0.0085*industri 0.0083*grow 0.0082*feed 0.0082*use 0.008*planet 0.0077*eat 0.0072*water 0.007*product 0.007*earth 0.007*problem 0.007*help 0.0069*would \n",
      "\n",
      "Topic: 1\n",
      "0.0751*anim 0.0407*make 0.0366*without 0.0212*world 0.0208*real 0.0206*better 0.0206*free 0.0206*slaughter 0.0197*futur 0.0184*help 0.0167*want 0.016*need 0.0158*grow 0.0156*planet 0.0147*peopl 0.013*food 0.013*save 0.0113*kill 0.0113*love 0.01*produc 0.01*harm 0.0095*support 0.0087*vegan 0.0082*human 0.0082*check 0.0078*chang 0.0076*cow 0.0074*billion 0.0063*cruelti 0.0063*word \n",
      "\n",
      "Topic: 2\n",
      "0.0413*food 0.0361*product 0.0221*plant 0.0212*industri 0.0189*market 0.0179*base 0.013*protein 0.0126*global 0.0123*scale 0.0096*suppli 0.0095*demand 0.0086*chain 0.0086*system 0.0086*process 0.0084*could 0.0079*altern 0.0074*sustain 0.007*cubiq 0.0068*consum 0.0067*toward 0.0065*like 0.0063*sale 0.0063*opportun 0.0061*investor 0.0061*anim 0.0061*commerci 0.0061*compani 0.0061*facil 0.006*next 0.0058*report \n",
      "\n",
      "Topic: 3\n",
      "0.0333*fish 0.0266*seafood 0.0199*shrimp 0.0179*ocean 0.0134*cell 0.0127*sustain 0.0112*year 0.0112*grow 0.0099*salmon 0.0097*tast 0.0095*product 0.0095*like 0.0095*bluenalu 0.0091*market 0.0084*know 0.0082*aquacultur 0.0082*cellular 0.0078*wild 0.0073*provid 0.0069*delici 0.0067*number 0.0067*healthi 0.0065*way 0.0065*also 0.0065*around 0.0063*chef 0.0063*farm 0.0063*consum 0.006*differ 0.006*plastic \n",
      "\n",
      "Topic: 4\n",
      "0.0325*chicken 0.0262*tast 0.0225*first 0.0219*burger 0.021*like 0.0184*base 0.0141*make 0.0141*cell 0.0134*product 0.013*look 0.0123*made 0.011*come 0.0106*plant 0.0104*good 0.0093*steak 0.0093*world 0.0091*anim 0.0087*meatbal 0.0082*bring 0.008*pork 0.0076*delici 0.0076*cook 0.0076*great 0.0069*duck 0.0069*wait 0.0067*test 0.0067*grown 0.0065*market 0.0063*lobster 0.0061*real \n",
      "\n",
      "Topic: 5\n",
      "0.0817*cell 0.0675*base 0.021*seafood 0.0189*product 0.0171*plant 0.0158*food 0.0158*meat 0.0138*futur 0.0132*look 0.0126*protein 0.0113*produc 0.0111*compani 0.0109*industri 0.0091*cellular 0.0088*great 0.0088*agricultur 0.0082*read 0.0074*learn 0.0074*shiok 0.0072*singapor 0.007*startup 0.0068*year 0.0066*forward 0.0066*stem 0.0066*technolog 0.0066*space 0.0064*dairi 0.0064*use 0.0062*start 0.0062*consum \n",
      "\n",
      "Topic: 6\n",
      "0.0273*compani 0.0253*invest 0.0246*startup 0.0212*fund 0.0208*rais 0.0204*announc 0.0193*food 0.0168*million 0.0157*protein 0.0154*base 0.0152*round 0.0152*world 0.0143*seri 0.0136*industri 0.0132*investor 0.0125*first 0.0118*cell 0.0118*singapor 0.0114*altern 0.0098*start 0.0096*tech 0.0094*close 0.0089*full 0.0087*shiok 0.008*ventur 0.008*congratul 0.0078*seed 0.0076*list 0.0074*develop 0.0074*excit \n",
      "\n",
      "Topic: 7\n",
      "0.0482*thank 0.0208*meat 0.0184*time 0.0171*year 0.0162*memphi 0.0145*join 0.0142*campaign 0.0138*great 0.0129*much 0.0123*think 0.0118*excit 0.0118*want 0.0116*contribut 0.0116*happi 0.0114*support 0.0105*question 0.0101*last 0.0101*peopl 0.0099*love 0.0094*work 0.0094*futur 0.009*realli 0.0085*pleas 0.0083*know 0.0081*check 0.0081*take "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:guidedlda:all zero row in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0079*answer 0.0079*would 0.0077*chanc 0.0077*talk \n",
      "\n",
      "Topic: 8\n",
      "0.027*speak 0.0222*join 0.0212*futur 0.0189*food 0.0179*confer 0.0179*panel 0.0154*discuss 0.0142*event 0.0138*th 0.0122*today 0.0113*summit 0.0105*present 0.0097*tomorrow 0.0093*live 0.0091*tune 0.0089*ceo 0.0089*next 0.0087*pitch 0.0076*come 0.0076*week 0.007*vote 0.007*meet 0.007*talk 0.0068*regist 0.0066*june 0.0066*scienc 0.0066*excit 0.0058*innov 0.0058*pm 0.0056*host \n",
      "\n",
      "Topic: 9\n",
      "0.0749*grown 0.0565*lab 0.0514*steak 0.0411*farm 0.0396*first 0.0314*cell 0.0302*aleph 0.0225*startup 0.0225*world 0.0181*isra 0.0148*compani 0.0133*free 0.0133*beef 0.013*slaughter 0.0125*start 0.0125*produc 0.0123*rais 0.0123*creat 0.0087*israel 0.0082*didier 0.0082*space 0.0082*could 0.0079*toubia 0.0079*million 0.0077*made 0.0074*shape 0.0069*experi 0.0066*viand 0.0061*print 0.0061*readi \n",
      "\n",
      "Topic: 10\n",
      "0.024*founder 0.0226*check 0.0214*featur 0.0214*great 0.0212*food 0.021*thank 0.0193*futur 0.0181*co 0.0171*interview 0.0163*talk 0.0149*articl 0.0147*latest 0.0126*watch 0.0124*podcast 0.012*discuss 0.0118*post 0.0112*read 0.0098*listen 0.0096*innov 0.0096*compani 0.0096*video 0.0096*good 0.0088*news 0.0088*today 0.0083*blog 0.0083*episod 0.0079*stori 0.0077*week 0.0075*amaz 0.0073*ceo \n",
      "\n",
      "Topic: 11\n",
      "0.0351*team 0.0247*work 0.0209*futur 0.0192*excit 0.0162*food 0.016*join 0.0157*look 0.0155*meat 0.0145*research 0.0107*make 0.0104*world 0.0104*scienc 0.0104*scientist 0.01*hire 0.01*bring 0.0097*welcom 0.0092*year 0.009*proud 0.0089*share 0.0085*memphi 0.0083*sustain 0.008*appli 0.0078*togeth 0.0077*help 0.0075*forward 0.0072*develop 0.007*amaz 0.0066*engin 0.0065*agricultur 0.0063*part topic_number\n",
      "0     866\n",
      "1     544\n",
      "2     506\n",
      "3     380\n",
      "4     493\n",
      "5     454\n",
      "6     535\n",
      "7     592\n",
      "8     559\n",
      "9     405\n",
      "10    593\n",
      "11    573\n",
      "Name: number, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "model = guidedlda.GuidedLDA(n_topics=12,n_iter=200,random_state=1,refresh=10,alpha=0.1,eta=0.01)\n",
    "vectorizer = CountVectorizer(min_df = 2)\n",
    "X = vectorizer.fit_transform(text['processed'])\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "word2id = dict((v,idx) for idx,v in enumerate(vocab))\n",
    "seed_topics = {}\n",
    "for t_id, st in enumerate(seed_topic_list):\n",
    "    for word in st:\n",
    "        seed_topics[word2id[word]] = t_id\n",
    "\n",
    "model.fit(X.toarray(),seed_topics=seed_topics,seed_confidence=0.35)\n",
    "topic_word = model.topic_word_\n",
    "n_top_words = 30\n",
    "vocab = tuple(vocab)\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    print('\\n')\n",
    "    print('Topic:',i)\n",
    "    words_probability = np.array(-topic_dist)\n",
    "    for index in range(n_top_words):\n",
    "        print(round(abs(np.sort(words_probability))[:(n_top_words)][index],4),'*',\n",
    "              np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1][index],sep='',end=' ')\n",
    "\n",
    "X = X.toarray()\n",
    "doc_topic = model.transform(X)\n",
    "topic_number = []\n",
    "number = []\n",
    "for i in range(len(doc_topic)-1):\n",
    "    topic_number.append(doc_topic[i].argmax())\n",
    "    number.append('1')\n",
    "data = pd.DataFrame(data=[i for i in topic_number],columns=['topic_number'])\n",
    "data['number'] = [i for i in number]\n",
    "print(data.groupby('topic_number')['number'].count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
