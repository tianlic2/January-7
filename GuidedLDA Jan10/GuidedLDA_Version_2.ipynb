{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tweets (excludes replies): 6501\n"
     ]
    }
   ],
   "source": [
    "#read data(exclude replies) \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import SnowballStemmer\n",
    "import pandas as pd\n",
    "\n",
    "text = pd.read_csv('Jan6(excludes replies).csv')\n",
    "print('The number of tweets (excludes replies):',len(text))\n",
    "#Text cleaning\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "import re\n",
    "stemmer = SnowballStemmer('english')\n",
    "porter = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += ['cultured','clean','cultivated','meat']\n",
    "b = []\n",
    "for i,u in text.iterrows():\n",
    "    a = []\n",
    "    word =''\n",
    "    for words in str(u['tweets']).split():\n",
    "        if '@' not in words: #remove @users\n",
    "            if '#' not in words:\n",
    "                if 'http' not in words: #remove URLs\n",
    "                    if'&amp' not in words: #remove symbol\n",
    "                        words = words.lower()# lower form\n",
    "                        words = re.sub(r'[^a-zA-Z]', ' ', words) #replace non-alphabets characters with space. From \"can't\" to \"can t\"\n",
    "                        if len(words)>3:\n",
    "                            word += (words+' ')\n",
    "    doc = ''\n",
    "    for token in word.split():\n",
    "        if token not in stop_words:\n",
    "            token = porter.stem(token) #root form\n",
    "            doc += (token+' ')\n",
    "    b.append(doc)\n",
    "text['processed']=[i for i in b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigram\n",
    "unigram = []\n",
    "unigram_list = []\n",
    "for index, i in text.iterrows():\n",
    "    unigram=''\n",
    "    for word in i['processed'].split():\n",
    "        unigram+= word+' '\n",
    "    unigram_list.append(unigram)\n",
    "\n",
    "\n",
    "import guidedlda\n",
    "import numpy as np\n",
    "import gensim\n",
    "\n",
    "seed_topic_list = [['environ','environment','carbon','climat','greenhous','emiss'], #Environmental Impact\n",
    "                   ['plant','vegan','vegetarian','animal','without','slaughter',\n",
    "                   'cruelti','welfar'],# Animal Welfare\n",
    "                  ['antibiot','antimicrobi','resist','safeti','contamin','consumpt','popul',\n",
    "                  'demand','secur','wast','industri','suppli','convent','regular','system'], #Food System\n",
    "                  ['seafood','sea','ocean','fish','aquacultur','shrimp'],#Seafood\n",
    "                  ['delici','tast','tender','textur','juici','test','chicken','duck','poultri','egg'],#Poultry\n",
    "                  ['cell','stem','cultur','muscl','biolog','divis'], #Process\n",
    "                  ['seri','fund','funder','rais','invest','investor','dollar','pound','announc', \n",
    "                  'happi','thrill','excit'], #Announcement\n",
    "                  ['thank','thankyou','support','shoutout','help'], #Appreciation\n",
    "                  ['confer','regist','live','symposium','stream','livestream'], #conference and summit\n",
    "                  ['market','consum','store','groceri'], # Market\n",
    "                  ['check','post','paper','interview','articl','blog','news','break'], #Media\n",
    "                  ['hire','join','team']]#Hiring Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:guidedlda:n_documents: 6501\n",
      "INFO:guidedlda:vocab_size: 3621\n",
      "INFO:guidedlda:n_words: 60068\n",
      "INFO:guidedlda:n_topics: 12\n",
      "INFO:guidedlda:n_iter: 200\n",
      "WARNING:guidedlda:all zero row in document-term matrix found\n",
      "INFO:guidedlda:<0> log likelihood: -694392\n",
      "INFO:guidedlda:<10> log likelihood: -490422\n",
      "INFO:guidedlda:<20> log likelihood: -474945\n",
      "INFO:guidedlda:<30> log likelihood: -467499\n",
      "INFO:guidedlda:<40> log likelihood: -463902\n",
      "INFO:guidedlda:<50> log likelihood: -461861\n",
      "INFO:guidedlda:<60> log likelihood: -460037\n",
      "INFO:guidedlda:<70> log likelihood: -458819\n",
      "INFO:guidedlda:<80> log likelihood: -457591\n",
      "INFO:guidedlda:<90> log likelihood: -456854\n",
      "INFO:guidedlda:<100> log likelihood: -456434\n",
      "INFO:guidedlda:<110> log likelihood: -455634\n",
      "INFO:guidedlda:<120> log likelihood: -455670\n",
      "INFO:guidedlda:<130> log likelihood: -455698\n",
      "INFO:guidedlda:<140> log likelihood: -455183\n",
      "INFO:guidedlda:<150> log likelihood: -455020\n",
      "INFO:guidedlda:<160> log likelihood: -454832\n",
      "INFO:guidedlda:<170> log likelihood: -454291\n",
      "INFO:guidedlda:<180> log likelihood: -454387\n",
      "INFO:guidedlda:<190> log likelihood: -453950\n",
      "INFO:guidedlda:<199> log likelihood: -453895\n",
      "WARNING:guidedlda:all zero row in document-term matrix found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: chang food anim world climat need sustain human solut health peopl produc environment global impact system agricultur reduc industri grow feed use planet eat water product earth problem help would\n",
      "Topic 1: anim make without world real better free slaughter futur help want need grow planet peopl food save kill love produc harm support vegan human check chang cow billion cruelti word\n",
      "Topic 2: food product plant industri market base protein global scale suppli demand chain system process could altern sustain cubiq consum toward like sale opportun investor anim commerci compani facil next report\n",
      "Topic 3: fish seafood shrimp ocean cell sustain year grow salmon tast product like bluenalu market know aquacultur cellular wild provid delici number healthi way also around chef farm consum differ plastic\n",
      "Topic 4: chicken tast first burger like base make cell product look made come plant good steak world anim meatbal bring pork delici cook great duck wait test grown market lobster real\n",
      "Topic 5: cell base seafood product plant food meat futur look protein produc compani industri cellular great agricultur read learn shiok singapor startup year forward stem technolog space dairi use start consum\n",
      "Topic 6: compani invest startup fund rais announc food million protein base round world seri industri investor first cell singapor altern start tech close full shiok ventur congratul seed list develop excit\n",
      "Topic 7: thank meat time year memphi join campaign great much think excit want contribut happi support question last peopl love work futur realli pleas know check take answer would chanc talk\n",
      "Topic 8: speak join futur food confer panel discuss event th today summit present tomorrow live tune ceo next pitch come week vote meet talk regist june scienc excit innov pm host\n",
      "Topic 9: grown lab steak farm first cell aleph startup world isra compani free beef slaughter start produc rais creat israel didier space could toubia million made shape experi viand print readi\n",
      "Topic 10: founder check featur great food thank futur co interview talk articl latest watch podcast discuss post read listen innov compani video good news today blog episod stori week amaz ceo\n",
      "Topic 11: team work futur excit food join look meat research make world scienc scientist hire bring welcom year proud share memphi sustain appli togeth help forward develop amaz engin agricultur part\n",
      "topic_number\n",
      "0     866\n",
      "1     544\n",
      "2     506\n",
      "3     380\n",
      "4     493\n",
      "5     454\n",
      "6     535\n",
      "7     592\n",
      "8     559\n",
      "9     405\n",
      "10    593\n",
      "11    573\n",
      "Name: number, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "model = guidedlda.GuidedLDA(n_topics=12,n_iter=200,random_state=1,refresh=10,alpha=0.1,eta=0.01)\n",
    "vectorizer = CountVectorizer(min_df = 2)\n",
    "X = vectorizer.fit_transform(text['processed'])\n",
    "\n",
    "vocab = vectorizer.get_feature_names()\n",
    "word2id = dict((v,idx) for idx,v in enumerate(vocab))\n",
    "seed_topics = {}\n",
    "for t_id, st in enumerate(seed_topic_list):\n",
    "    for word in st:\n",
    "        seed_topics[word2id[word]] = t_id\n",
    "\n",
    "model.fit(X.toarray(),seed_topics=seed_topics,seed_confidence=0.35)\n",
    "topic_word = model.topic_word_\n",
    "n_top_words = 30\n",
    "vocab = tuple(vocab)\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    print('Topic {}: {}'.format(i, ' '.join(topic_words)))\n",
    "\n",
    "X = X.toarray()\n",
    "doc_topic = model.transform(X)\n",
    "topic_number = []\n",
    "number = []\n",
    "for i in range(len(doc_topic)-1):\n",
    "    topic_number.append(doc_topic[i].argmax())\n",
    "    number.append('1')\n",
    "data = pd.DataFrame(data=[i for i in topic_number],columns=['topic_number'])\n",
    "data['number'] = [i for i in number]\n",
    "print(data.groupby('topic_number')['number'].count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
